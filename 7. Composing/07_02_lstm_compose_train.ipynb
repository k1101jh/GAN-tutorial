{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 작곡: 음악을 생성하는 모델을 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np\n",
    "from music21 import note, chord\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from music21 import corpus, converter\n",
    "\n",
    "from RNNAttention import RNNAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals = range(1)\n",
    "seq_len = 32\n",
    "embed_size = 100\n",
    "rnn_units = 256\n",
    "batch_size = 32\n",
    "use_attention = True\n",
    "epochs = 20000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "mode = 'build'\n",
    "# mode = 'load'\n",
    "\n",
    "data_folder = '../data/cello'\n",
    "image_save_folder = './images/lstm_compose'\n",
    "store_folder = './store'\n",
    "model_save_path = './lstm_compose.pth'\n",
    "\n",
    "os.makedirs(image_save_folder, exist_ok=True)\n",
    "os.makedirs(store_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 악보 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_music_list(data_folder):\n",
    "    if data_folder == 'chorales':\n",
    "        file_list = ['bwv' + str(x['bwv']) for x in corpus.chorales.ChoraleList().byBWV.values()]\n",
    "        parser = corpus\n",
    "    else:\n",
    "        file_list = glob.glob(os.path.join(data_folder, \"*.mid\"))\n",
    "        parser = converter\n",
    "    \n",
    "    return file_list, parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if mode == 'build':\n",
    "    music_list, parser = get_music_list(data_folder)\n",
    "    print(len(music_list), 'files in total')\n",
    "\n",
    "    notes = []\n",
    "    durations = []\n",
    "\n",
    "    for i, file in enumerate(music_list):\n",
    "        print(i + 1, \"Parsing %s\" % file)\n",
    "        original_score = parser.parse(file).chordify()\n",
    "        \n",
    "        for interval in intervals:\n",
    "            score = original_score.transpose(interval)\n",
    "            \n",
    "            notes.extend(['START'] * seq_len)\n",
    "            durations.extend([0] * seq_len)\n",
    "            \n",
    "            for element in score.flat:\n",
    "                if isinstance(element, note.Note):\n",
    "                    if element.isRest:\n",
    "                        notes.append(str(element.name))\n",
    "                        durations.append(element.duration.quarterLength)\n",
    "                    else:\n",
    "                        notes.append(str(element.nameWithOctave))\n",
    "                        durations.append(element.duration.quarterLength)\n",
    "\n",
    "                if isinstance(element, chord.Chord):\n",
    "                    notes.append('.'.join(n.nameWithOctave for n in element.pitches))\n",
    "                    durations.append(element.duration.quarterLength)\n",
    "\n",
    "    with open(os.path.join(store_folder, 'notes'), 'wb') as f:\n",
    "        pickle.dump(notes, f) #['G2', 'D3', 'B3', 'A3', 'B3', 'D3', 'B3', 'D3', 'G2',...]\n",
    "    with open(os.path.join(store_folder, 'durations'), 'wb') as f:\n",
    "        pickle.dump(durations, f)\n",
    "else:\n",
    "    with open(os.path.join(store_folder, 'notes'), 'rb') as f:\n",
    "        notes = pickle.load(f) #['G2', 'D3', 'B3', 'A3', 'B3', 'D3', 'B3', 'D3', 'G2',...]\n",
    "    with open(os.path.join(store_folder, 'durations'), 'rb') as f:\n",
    "        durations = pickle.load(f) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 룩업 테이블 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distinct(elements):\n",
    "    element_names = sorted(set(elements))\n",
    "    n_elements = len(element_names)\n",
    "    return (element_names, n_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lookups(element_names):\n",
    "    element_to_int = dict((element, number) for number, element in enumerate(element_names))\n",
    "    int_to_element = dict((number, element) for number, element in enumerate(element_names))\n",
    "    \n",
    "    return (element_to_int, int_to_element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_names, n_notes = get_distinct(notes)\n",
    "duration_names, n_durations = get_distinct(durations)\n",
    "distincts = [note_names, n_notes, duration_names, n_durations]\n",
    "\n",
    "with open(os.path.join(store_folder, 'distincts'), 'wb') as f:\n",
    "    pickle.dump(distincts, f)\n",
    "    \n",
    "note_to_int, int_to_note = create_lookups(note_names)\n",
    "duration_to_int, int_to_duration = create_lookups(duration_names)\n",
    "lookups = [note_to_int, int_to_note, duration_to_int, int_to_duration]\n",
    "\n",
    "with open(os.path.join(store_folder, 'lookups'), 'wb') as f:\n",
    "    pickle.dump(lookups, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nnote_to_int')\n",
    "note_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nduration_to_int')\n",
    "duration_to_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망에 사용할 시퀀스 준비하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, notes, durations, lookups, distincts, seq_len=32):\n",
    "        note_to_int, int_to_note, duration_to_int, int_to_duration = lookups\n",
    "        note_names, n_notes, duration_names, n_durations = distincts\n",
    "        \n",
    "        self.dataset_len = len(notes) - seq_len\n",
    "         \n",
    "        input_shape = [self.dataset_len, seq_len]\n",
    "        \n",
    "        self.notes_network_input = torch.zeros(input_shape, dtype=torch.int64)\n",
    "        self.notes_network_output = torch.zeros(self.dataset_len, dtype=torch.int64)\n",
    "        self.durations_network_input = torch.zeros(input_shape, dtype=torch.int64)\n",
    "        self.durations_network_output = torch.zeros(self.dataset_len, dtype=torch.int64)\n",
    "        \n",
    "        for i in range(self.dataset_len):\n",
    "            notes_sequence_in = notes[i:i + seq_len]\n",
    "            notes_sequence_out = notes[i + seq_len]\n",
    "            self.notes_network_input[i] = torch.FloatTensor([note_to_int[char] for char in notes_sequence_in])\n",
    "            self.notes_network_output[i] = note_to_int[notes_sequence_out]\n",
    "            \n",
    "            durations_sequence_in = durations[i:i + seq_len]\n",
    "            durations_sequence_out = durations[i + seq_len]\n",
    "            self.durations_network_input[i] = torch.FloatTensor([duration_to_int[char] for char in durations_sequence_in])\n",
    "            self.durations_network_output[i] = duration_to_int[durations_sequence_out]\n",
    "            \n",
    "        n_patterns = len(self.notes_network_input)\n",
    "        \n",
    "        self.notes_network_input = torch.reshape(self.notes_network_input, (n_patterns, seq_len))\n",
    "        self.durations_network_input = torch.reshape(self.durations_network_input, (n_patterns, seq_len))\n",
    "        # network_input = [self.notes_network_input, self.durations_network_input]\n",
    "        \n",
    "        self.notes_network_output = F.one_hot(self.notes_network_output, num_classes=n_notes).double()\n",
    "        self.durations_network_output = F.one_hot(self.durations_network_output, num_classes=n_durations).double()\n",
    "        # network_output = [self.notes_network_output, self.durations_network_output]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return ([self.notes_network_input[idx], self.durations_network_input[idx]],\n",
    "                [self.notes_network_output[idx], self.durations_network_output[idx]])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.dataset_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(notes, durations, lookups, distincts, seq_len)\n",
    "validation_split_ratio = 0.8\n",
    "train_size = int(len(dataset) * validation_split_ratio)\n",
    "val_size = int(len(dataset) - train_size)\n",
    "\n",
    "train_set, val_set = random_split(dataset, [train_size, val_size])\n",
    "train_dataloader = DataLoader(train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_dataloader = DataLoader(val_set, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': train_dataloader,\n",
    "    'val': val_dataloader,\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    'train': len(train_set),\n",
    "    'val': len(val_set),\n",
    "}\n",
    "\n",
    "print('dataset size')\n",
    "print(len(dataset))\n",
    "print('train set size')\n",
    "print(len(train_set))\n",
    "print('validation set size')\n",
    "print(len(val_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sample, output_sample = next(iter(train_dataloader))\n",
    "print('pitch input')\n",
    "print(input_sample[0][0])\n",
    "print('duration input')\n",
    "print(input_sample[1][0])\n",
    "print('pitch output')\n",
    "print(output_sample[0][0])\n",
    "print('duration output')\n",
    "print(output_sample[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNAttention(n_notes, n_durations, embed_size, rnn_units, seq_len, use_attention)\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 신경망 훈련하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "critic = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pitch_losses = []\n",
    "train_duration_losses = []\n",
    "train_losses = []\n",
    "\n",
    "val_pitch_losses = []\n",
    "val_duration_losses = []\n",
    "val_losses = []\n",
    "\n",
    "best_loss = 1e4\n",
    "patience_limit = 10\n",
    "patience = 0\n",
    "\n",
    "for epoch in range(epochs):    \n",
    "    for phase in ['train', 'val']:\n",
    "        if phase == 'train':\n",
    "            model.train()\n",
    "        else:\n",
    "            model.eval()\n",
    "            \n",
    "        start_time = time.time()\n",
    "    \n",
    "        epoch_pitch_loss = 0.0\n",
    "        epoch_duration_loss = 0.0\n",
    "        epoch_loss = 0.0\n",
    "        for inputs, labels in dataloaders[phase]:\n",
    "            pitch_labels = labels[0].to(device)\n",
    "            duration_labels = labels[1].to(device)\n",
    "            \n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                output, _ = model(inputs)\n",
    "\n",
    "                pitch_outputs = output[0]\n",
    "                duration_outputs = output[1]\n",
    "                \n",
    "                pitch_loss = 0.0\n",
    "                duration_loss = 0.0\n",
    "                for i in range(pitch_outputs.shape[0]):\n",
    "                    pitch_loss += critic(pitch_outputs[i], pitch_labels[i])\n",
    "                    duration_loss += critic(duration_outputs[i], duration_labels[i])\n",
    "                    \n",
    "                pitch_loss /= pitch_outputs.shape[0]\n",
    "                duration_loss /= duration_outputs.shape[0]\n",
    "                \n",
    "                loss = pitch_loss + duration_loss\n",
    "                \n",
    "                if phase == 'train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "            epoch_pitch_loss += pitch_loss.item() * inputs[0].size(0)\n",
    "            epoch_duration_loss += duration_loss.item() * inputs[0].size(0)\n",
    "            epoch_loss += loss.item() * inputs[0].size(0)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "    \n",
    "        epoch_pitch_loss /= dataset_sizes[phase]\n",
    "        epoch_duration_loss /= dataset_sizes[phase]\n",
    "        epoch_loss /= dataset_sizes[phase]\n",
    "        \n",
    "        if phase == 'train':\n",
    "            train_pitch_losses.append(epoch_pitch_loss)\n",
    "            train_duration_losses.append(epoch_duration_loss)\n",
    "            train_losses.append(epoch_loss)\n",
    "        else:\n",
    "            val_pitch_losses.append(epoch_pitch_loss)\n",
    "            val_duration_losses.append(epoch_duration_loss)\n",
    "            val_losses.append(epoch_loss)\n",
    "    \n",
    "        print(\"[Epoch %d/%d] [Phase: %s] [loss: %.4f, pitch loss: %.4f, duration loss: %.4f] time: %.4f\"\\\n",
    "            % (epoch, epochs, phase,\n",
    "            epoch_loss, epoch_pitch_loss, epoch_duration_loss,\n",
    "            elapsed_time))\n",
    "            \n",
    "    # validation 단계의 loss 비교\n",
    "    if(epoch_loss < best_loss):\n",
    "        patience = 0\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "    else:\n",
    "        patience += 1\n",
    "        if(patience >= patience_limit):\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.plot([x for x in train_pitch_losses], color='black', linewidth=1)\n",
    "plt.plot([x for x in train_duration_losses], color='green', linewidth=1)\n",
    "plt.plot([x for x in train_losses], color='red', linewidth=1)\n",
    "\n",
    "plt.xlabel('epoch', fontsize=18)\n",
    "plt.ylabel('loss', fontsize=16)\n",
    "\n",
    "plt.ylim(0, 5)\n",
    "\n",
    "plt.savefig(os.path.join(image_save_folder, 'train_loss_graph.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.plot([x for x in val_pitch_losses], color='black', linewidth=1)\n",
    "plt.plot([x for x in val_duration_losses], color='green', linewidth=1)\n",
    "plt.plot([x for x in val_losses], color='red', linewidth=1)\n",
    "\n",
    "plt.xlabel('epoch', fontsize=18)\n",
    "plt.ylabel('loss', fontsize=16)\n",
    "\n",
    "plt.ylim(0, 5)\n",
    "\n",
    "plt.savefig(os.path.join(image_save_folder, 'val_loss_graph.png'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('gan_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d96dd6218930269b4cd8feca07fdf09d8d639cf486d090b424fe3934a00b48b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
