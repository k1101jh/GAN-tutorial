{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from auto_encoder import AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 200\n",
    "batch_size = 32\n",
    "learning_rate = 3e-4\n",
    "decay_factor = 0.99\n",
    "model_save_path = \"./auto_encoder_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(root='./data/',\n",
    "                            train=True,\n",
    "                            download=True,\n",
    "                            transform=transforms.ToTensor())\n",
    "\n",
    "indices = torch.arange(1000)\n",
    "train_data_1k = data_utils.Subset(train_data, indices)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data_1k,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4,\n",
    "                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoEncoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (8): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (linear1): Linear(in_features=3136, out_features=2, bias=True)\n",
      "  (linear2): Linear(in_features=2, out_features=3136, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (2): ConvTranspose2d(64, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (4): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (6): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = AutoEncoder().to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer,\n",
    "                                        lr_lambda=(lambda epoch: decay_factor ** epoch))\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r_loss(y_true, y_pred):\n",
    "    return torch.mean(torch.mean(torch.square(y_true - y_pred), axis=[1, 2, 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000\tLoss: 0.05499\n",
      "Epoch 001\tLoss: 0.05331\n",
      "Epoch 002\tLoss: 0.05161\n",
      "Epoch 003\tLoss: 0.05065\n",
      "Epoch 004\tLoss: 0.04971\n",
      "Epoch 005\tLoss: 0.04893\n",
      "Epoch 006\tLoss: 0.04826\n",
      "Epoch 007\tLoss: 0.04797\n",
      "Epoch 008\tLoss: 0.04744\n",
      "Epoch 009\tLoss: 0.04696\n",
      "Epoch 010\tLoss: 0.04671\n",
      "Epoch 011\tLoss: 0.04617\n",
      "Epoch 012\tLoss: 0.04580\n",
      "Epoch 013\tLoss: 0.04556\n",
      "Epoch 014\tLoss: 0.04513\n",
      "Epoch 015\tLoss: 0.04504\n",
      "Epoch 016\tLoss: 0.04454\n",
      "Epoch 017\tLoss: 0.04438\n",
      "Epoch 018\tLoss: 0.04422\n",
      "Epoch 019\tLoss: 0.04375\n",
      "Epoch 020\tLoss: 0.04348\n",
      "Epoch 021\tLoss: 0.04325\n",
      "Epoch 022\tLoss: 0.04301\n",
      "Epoch 023\tLoss: 0.04277\n",
      "Epoch 024\tLoss: 0.04260\n",
      "Epoch 025\tLoss: 0.04235\n",
      "Epoch 026\tLoss: 0.04208\n",
      "Epoch 027\tLoss: 0.04186\n",
      "Epoch 028\tLoss: 0.04190\n",
      "Epoch 029\tLoss: 0.04189\n",
      "Epoch 030\tLoss: 0.04145\n",
      "Epoch 031\tLoss: 0.04128\n",
      "Epoch 032\tLoss: 0.04115\n",
      "Epoch 033\tLoss: 0.04077\n",
      "Epoch 034\tLoss: 0.04063\n",
      "Epoch 035\tLoss: 0.04057\n",
      "Epoch 036\tLoss: 0.04036\n",
      "Epoch 037\tLoss: 0.04044\n",
      "Epoch 038\tLoss: 0.04007\n",
      "Epoch 039\tLoss: 0.04004\n",
      "Epoch 040\tLoss: 0.03985\n",
      "Epoch 041\tLoss: 0.03967\n",
      "Epoch 042\tLoss: 0.03949\n",
      "Epoch 043\tLoss: 0.03939\n",
      "Epoch 044\tLoss: 0.03914\n",
      "Epoch 045\tLoss: 0.03901\n",
      "Epoch 046\tLoss: 0.03893\n",
      "Epoch 047\tLoss: 0.03887\n",
      "Epoch 048\tLoss: 0.03868\n",
      "Epoch 049\tLoss: 0.03880\n",
      "Epoch 050\tLoss: 0.03841\n",
      "Epoch 051\tLoss: 0.03836\n",
      "Epoch 052\tLoss: 0.03812\n",
      "Epoch 053\tLoss: 0.03818\n",
      "Epoch 054\tLoss: 0.03809\n",
      "Epoch 055\tLoss: 0.03813\n",
      "Epoch 056\tLoss: 0.03775\n",
      "Epoch 057\tLoss: 0.03768\n",
      "Epoch 058\tLoss: 0.03767\n",
      "Epoch 059\tLoss: 0.03752\n",
      "Epoch 060\tLoss: 0.03748\n",
      "Epoch 061\tLoss: 0.03742\n",
      "Epoch 062\tLoss: 0.03721\n",
      "Epoch 063\tLoss: 0.03725\n",
      "Epoch 064\tLoss: 0.03708\n",
      "Epoch 065\tLoss: 0.03693\n",
      "Epoch 066\tLoss: 0.03700\n",
      "Epoch 067\tLoss: 0.03670\n",
      "Epoch 068\tLoss: 0.03676\n",
      "Epoch 069\tLoss: 0.03666\n",
      "Epoch 070\tLoss: 0.03668\n",
      "Epoch 071\tLoss: 0.03671\n",
      "Epoch 072\tLoss: 0.03646\n",
      "Epoch 073\tLoss: 0.03633\n",
      "Epoch 074\tLoss: 0.03623\n",
      "Epoch 075\tLoss: 0.03615\n",
      "Epoch 076\tLoss: 0.03618\n",
      "Epoch 077\tLoss: 0.03613\n",
      "Epoch 078\tLoss: 0.03605\n",
      "Epoch 079\tLoss: 0.03583\n",
      "Epoch 080\tLoss: 0.03586\n",
      "Epoch 081\tLoss: 0.03575\n",
      "Epoch 082\tLoss: 0.03576\n",
      "Epoch 083\tLoss: 0.03569\n",
      "Epoch 084\tLoss: 0.03557\n",
      "Epoch 085\tLoss: 0.03558\n",
      "Epoch 086\tLoss: 0.03542\n",
      "Epoch 087\tLoss: 0.03541\n",
      "Epoch 088\tLoss: 0.03545\n",
      "Epoch 089\tLoss: 0.03529\n",
      "Epoch 090\tLoss: 0.03525\n",
      "Epoch 091\tLoss: 0.03528\n",
      "Epoch 092\tLoss: 0.03510\n",
      "Epoch 093\tLoss: 0.03511\n",
      "Epoch 094\tLoss: 0.03498\n",
      "Epoch 095\tLoss: 0.03498\n",
      "Epoch 096\tLoss: 0.03502\n",
      "Epoch 097\tLoss: 0.03487\n",
      "Epoch 098\tLoss: 0.03483\n",
      "Epoch 099\tLoss: 0.03476\n",
      "Epoch 100\tLoss: 0.03482\n",
      "Epoch 101\tLoss: 0.03473\n",
      "Epoch 102\tLoss: 0.03457\n",
      "Epoch 103\tLoss: 0.03457\n",
      "Epoch 104\tLoss: 0.03459\n",
      "Epoch 105\tLoss: 0.03455\n",
      "Epoch 106\tLoss: 0.03445\n",
      "Epoch 107\tLoss: 0.03445\n",
      "Epoch 108\tLoss: 0.03437\n",
      "Epoch 109\tLoss: 0.03437\n",
      "Epoch 110\tLoss: 0.03416\n",
      "Epoch 111\tLoss: 0.03416\n",
      "Epoch 112\tLoss: 0.03416\n",
      "Epoch 113\tLoss: 0.03418\n",
      "Epoch 114\tLoss: 0.03404\n",
      "Epoch 115\tLoss: 0.03399\n",
      "Epoch 116\tLoss: 0.03405\n",
      "Epoch 117\tLoss: 0.03396\n",
      "Epoch 118\tLoss: 0.03388\n",
      "Epoch 119\tLoss: 0.03380\n",
      "Epoch 120\tLoss: 0.03383\n",
      "Epoch 121\tLoss: 0.03380\n",
      "Epoch 122\tLoss: 0.03380\n",
      "Epoch 123\tLoss: 0.03373\n",
      "Epoch 124\tLoss: 0.03373\n",
      "Epoch 125\tLoss: 0.03373\n",
      "Epoch 126\tLoss: 0.03356\n",
      "Epoch 127\tLoss: 0.03366\n",
      "Epoch 128\tLoss: 0.03358\n",
      "Epoch 129\tLoss: 0.03353\n",
      "Epoch 130\tLoss: 0.03352\n",
      "Epoch 131\tLoss: 0.03350\n",
      "Epoch 132\tLoss: 0.03341\n",
      "Epoch 133\tLoss: 0.03338\n",
      "Epoch 134\tLoss: 0.03335\n",
      "Epoch 135\tLoss: 0.03332\n",
      "Epoch 136\tLoss: 0.03326\n",
      "Epoch 137\tLoss: 0.03331\n",
      "Epoch 138\tLoss: 0.03319\n",
      "Epoch 139\tLoss: 0.03317\n",
      "Epoch 140\tLoss: 0.03312\n",
      "Epoch 141\tLoss: 0.03319\n",
      "Epoch 142\tLoss: 0.03313\n",
      "Epoch 143\tLoss: 0.03314\n",
      "Epoch 144\tLoss: 0.03313\n",
      "Epoch 145\tLoss: 0.03302\n",
      "Epoch 146\tLoss: 0.03296\n",
      "Epoch 147\tLoss: 0.03294\n",
      "Epoch 148\tLoss: 0.03286\n",
      "Epoch 149\tLoss: 0.03291\n",
      "Epoch 150\tLoss: 0.03288\n",
      "Epoch 151\tLoss: 0.03288\n",
      "Epoch 152\tLoss: 0.03289\n",
      "Epoch 153\tLoss: 0.03284\n",
      "Epoch 154\tLoss: 0.03283\n",
      "Epoch 155\tLoss: 0.03280\n",
      "Epoch 156\tLoss: 0.03276\n",
      "Epoch 157\tLoss: 0.03277\n",
      "Epoch 158\tLoss: 0.03267\n",
      "Epoch 159\tLoss: 0.03264\n",
      "Epoch 160\tLoss: 0.03267\n",
      "Epoch 161\tLoss: 0.03266\n",
      "Epoch 162\tLoss: 0.03261\n",
      "Epoch 163\tLoss: 0.03257\n",
      "Epoch 164\tLoss: 0.03257\n",
      "Epoch 165\tLoss: 0.03250\n",
      "Epoch 166\tLoss: 0.03254\n",
      "Epoch 167\tLoss: 0.03250\n",
      "Epoch 168\tLoss: 0.03246\n",
      "Epoch 169\tLoss: 0.03241\n",
      "Epoch 170\tLoss: 0.03241\n",
      "Epoch 171\tLoss: 0.03231\n",
      "Epoch 172\tLoss: 0.03235\n",
      "Epoch 173\tLoss: 0.03240\n",
      "Epoch 174\tLoss: 0.03233\n",
      "Epoch 175\tLoss: 0.03228\n",
      "Epoch 176\tLoss: 0.03230\n",
      "Epoch 177\tLoss: 0.03228\n",
      "Epoch 178\tLoss: 0.03227\n",
      "Epoch 179\tLoss: 0.03221\n",
      "Epoch 180\tLoss: 0.03219\n",
      "Epoch 181\tLoss: 0.03215\n",
      "Epoch 182\tLoss: 0.03213\n",
      "Epoch 183\tLoss: 0.03211\n",
      "Epoch 184\tLoss: 0.03217\n",
      "Epoch 185\tLoss: 0.03212\n",
      "Epoch 186\tLoss: 0.03204\n",
      "Epoch 187\tLoss: 0.03209\n",
      "Epoch 188\tLoss: 0.03208\n",
      "Epoch 189\tLoss: 0.03208\n",
      "Epoch 190\tLoss: 0.03205\n",
      "Epoch 191\tLoss: 0.03199\n",
      "Epoch 192\tLoss: 0.03196\n",
      "Epoch 193\tLoss: 0.03195\n",
      "Epoch 194\tLoss: 0.03199\n",
      "Epoch 195\tLoss: 0.03195\n",
      "Epoch 196\tLoss: 0.03194\n",
      "Epoch 197\tLoss: 0.03190\n",
      "Epoch 198\tLoss: 0.03188\n",
      "Epoch 199\tLoss: 0.03186\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs = model(inputs)\n",
    "            loss = F.mse_loss(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "    scheduler.step()\n",
    "    epoch_loss = running_loss / len(train_data_1k)\n",
    "    print('Epoch {0:03d}\\tLoss: {1:0.5f}'.format(epoch, epoch_loss))\n",
    "\n",
    "torch.save(model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86945038929e12337cb19f89597171695d197df89cc82498a4710d2d3ad0cdf4"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('gan_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
