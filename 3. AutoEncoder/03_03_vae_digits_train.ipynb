{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 변이형 오토인코더 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 임포트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from vae_auto_encoder import VAEAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "r_loss_factor = 1000\n",
    "decay_factor = 0.99\n",
    "data_path = '../data/'\n",
    "model_save_path = \"./vae_digits_model.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 적재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(root=data_path,\n",
    "                            train=True,\n",
    "                            download=True,\n",
    "                            transform=transforms.ToTensor())\n",
    "dataset_size = len(train_data)\n",
    "train_loader = DataLoader(dataset=train_data,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4,\n",
    "                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 60000\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset size:\", dataset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAEAutoEncoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (8): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mu_layer): Linear(in_features=3136, out_features=2, bias=True)\n",
      "  (log_var_layer): Linear(in_features=3136, out_features=2, bias=True)\n",
      "  (linear): Linear(in_features=2, out_features=3136, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (2): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
      "    (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (4): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
      "    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (6): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = VAEAutoEncoder(num_layers=4,\n",
    "                       encoder_channels=[1, 32, 64, 64, 64],\n",
    "                       encoder_kernel_sizes=[3, 3, 3, 3],\n",
    "                       encoder_strides=[1, 2, 2, 1],\n",
    "                       decoder_channels=[64, 64, 64, 32, 1],\n",
    "                       decoder_kernel_sizes=[3, 3, 3, 3],\n",
    "                       decoder_strides=[1, 2, 2, 1],\n",
    "                       linear_sizes=[3136, 2],\n",
    "                       view_size=[-1, 64, 7, 7],\n",
    "                       use_batch_norm=False,\n",
    "                       use_dropout=False).to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer,\n",
    "                                        lr_lambda=(lambda epoch: decay_factor ** epoch))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_kl_loss(mu, log_var):\n",
    "    kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), axis=1)\n",
    "    return torch.mean(kl_loss)\n",
    "\n",
    "r_criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001\tLoss: 57.06260\tr_loss: 53.84807\tkl_loss: 3.21454\n",
      "Epoch 002\tLoss: 49.17750\tr_loss: 44.72828\tkl_loss: 4.44922\n",
      "Epoch 003\tLoss: 47.68091\tr_loss: 42.94135\tkl_loss: 4.73956\n",
      "Epoch 004\tLoss: 46.83676\tr_loss: 41.95326\tkl_loss: 4.88349\n",
      "Epoch 005\tLoss: 46.32283\tr_loss: 41.34881\tkl_loss: 4.97402\n",
      "Epoch 006\tLoss: 45.92388\tr_loss: 40.84339\tkl_loss: 5.08049\n",
      "Epoch 007\tLoss: 45.59897\tr_loss: 40.47061\tkl_loss: 5.12836\n",
      "Epoch 008\tLoss: 45.30886\tr_loss: 40.12167\tkl_loss: 5.18719\n",
      "Epoch 009\tLoss: 45.05550\tr_loss: 39.82849\tkl_loss: 5.22701\n",
      "Epoch 010\tLoss: 44.87323\tr_loss: 39.58311\tkl_loss: 5.29011\n",
      "Epoch 011\tLoss: 44.68638\tr_loss: 39.37386\tkl_loss: 5.31252\n",
      "Epoch 012\tLoss: 44.49468\tr_loss: 39.14780\tkl_loss: 5.34689\n",
      "Epoch 013\tLoss: 44.38228\tr_loss: 39.00866\tkl_loss: 5.37362\n",
      "Epoch 014\tLoss: 44.19775\tr_loss: 38.79111\tkl_loss: 5.40664\n",
      "Epoch 015\tLoss: 44.08610\tr_loss: 38.65552\tkl_loss: 5.43058\n",
      "Epoch 016\tLoss: 43.98302\tr_loss: 38.54708\tkl_loss: 5.43594\n",
      "Epoch 017\tLoss: 43.81777\tr_loss: 38.34772\tkl_loss: 5.47005\n",
      "Epoch 018\tLoss: 43.73443\tr_loss: 38.24773\tkl_loss: 5.48670\n",
      "Epoch 019\tLoss: 43.63469\tr_loss: 38.12545\tkl_loss: 5.50924\n",
      "Epoch 020\tLoss: 43.53414\tr_loss: 38.00477\tkl_loss: 5.52937\n",
      "Epoch 021\tLoss: 43.42467\tr_loss: 37.88758\tkl_loss: 5.53708\n",
      "Epoch 022\tLoss: 43.32563\tr_loss: 37.76650\tkl_loss: 5.55913\n",
      "Epoch 023\tLoss: 43.26521\tr_loss: 37.68277\tkl_loss: 5.58244\n",
      "Epoch 024\tLoss: 43.18952\tr_loss: 37.59578\tkl_loss: 5.59374\n",
      "Epoch 025\tLoss: 43.07957\tr_loss: 37.47319\tkl_loss: 5.60638\n",
      "Epoch 026\tLoss: 43.00765\tr_loss: 37.38997\tkl_loss: 5.61767\n",
      "Epoch 027\tLoss: 42.95024\tr_loss: 37.31470\tkl_loss: 5.63554\n",
      "Epoch 028\tLoss: 42.85798\tr_loss: 37.22393\tkl_loss: 5.63404\n",
      "Epoch 029\tLoss: 42.82900\tr_loss: 37.16924\tkl_loss: 5.65976\n",
      "Epoch 030\tLoss: 42.71675\tr_loss: 37.04334\tkl_loss: 5.67341\n",
      "Epoch 031\tLoss: 42.67147\tr_loss: 37.00191\tkl_loss: 5.66956\n",
      "Epoch 032\tLoss: 42.62029\tr_loss: 36.92818\tkl_loss: 5.69211\n",
      "Epoch 033\tLoss: 42.55962\tr_loss: 36.86271\tkl_loss: 5.69691\n",
      "Epoch 034\tLoss: 42.51265\tr_loss: 36.79670\tkl_loss: 5.71595\n",
      "Epoch 035\tLoss: 42.45619\tr_loss: 36.72852\tkl_loss: 5.72767\n",
      "Epoch 036\tLoss: 42.37564\tr_loss: 36.64203\tkl_loss: 5.73362\n",
      "Epoch 037\tLoss: 42.33033\tr_loss: 36.59291\tkl_loss: 5.73743\n",
      "Epoch 038\tLoss: 42.28232\tr_loss: 36.53344\tkl_loss: 5.74888\n",
      "Epoch 039\tLoss: 42.22528\tr_loss: 36.47026\tkl_loss: 5.75503\n",
      "Epoch 040\tLoss: 42.16430\tr_loss: 36.40516\tkl_loss: 5.75914\n",
      "Epoch 041\tLoss: 42.14929\tr_loss: 36.36702\tkl_loss: 5.78227\n",
      "Epoch 042\tLoss: 42.09584\tr_loss: 36.30259\tkl_loss: 5.79325\n",
      "Epoch 043\tLoss: 42.03314\tr_loss: 36.23832\tkl_loss: 5.79482\n",
      "Epoch 044\tLoss: 42.02632\tr_loss: 36.21441\tkl_loss: 5.81191\n",
      "Epoch 045\tLoss: 41.96215\tr_loss: 36.14908\tkl_loss: 5.81307\n",
      "Epoch 046\tLoss: 41.92235\tr_loss: 36.10192\tkl_loss: 5.82043\n",
      "Epoch 047\tLoss: 41.87441\tr_loss: 36.04190\tkl_loss: 5.83251\n",
      "Epoch 048\tLoss: 41.82222\tr_loss: 35.98288\tkl_loss: 5.83934\n",
      "Epoch 049\tLoss: 41.79903\tr_loss: 35.95684\tkl_loss: 5.84219\n",
      "Epoch 050\tLoss: 41.75999\tr_loss: 35.90966\tkl_loss: 5.85033\n",
      "Epoch 051\tLoss: 41.73606\tr_loss: 35.88502\tkl_loss: 5.85104\n",
      "Epoch 052\tLoss: 41.68008\tr_loss: 35.83226\tkl_loss: 5.84782\n",
      "Epoch 053\tLoss: 41.62123\tr_loss: 35.75269\tkl_loss: 5.86854\n",
      "Epoch 054\tLoss: 41.60625\tr_loss: 35.74191\tkl_loss: 5.86434\n",
      "Epoch 055\tLoss: 41.57208\tr_loss: 35.69050\tkl_loss: 5.88158\n",
      "Epoch 056\tLoss: 41.57352\tr_loss: 35.67478\tkl_loss: 5.89874\n",
      "Epoch 057\tLoss: 41.51454\tr_loss: 35.61570\tkl_loss: 5.89883\n",
      "Epoch 058\tLoss: 41.46303\tr_loss: 35.56940\tkl_loss: 5.89363\n",
      "Epoch 059\tLoss: 41.43314\tr_loss: 35.52175\tkl_loss: 5.91139\n",
      "Epoch 060\tLoss: 41.43142\tr_loss: 35.50737\tkl_loss: 5.92406\n",
      "Epoch 061\tLoss: 41.37274\tr_loss: 35.44171\tkl_loss: 5.93103\n",
      "Epoch 062\tLoss: 41.34138\tr_loss: 35.41673\tkl_loss: 5.92465\n",
      "Epoch 063\tLoss: 41.31366\tr_loss: 35.38849\tkl_loss: 5.92518\n",
      "Epoch 064\tLoss: 41.31081\tr_loss: 35.37544\tkl_loss: 5.93538\n",
      "Epoch 065\tLoss: 41.26348\tr_loss: 35.32815\tkl_loss: 5.93533\n",
      "Epoch 066\tLoss: 41.24134\tr_loss: 35.29718\tkl_loss: 5.94416\n",
      "Epoch 067\tLoss: 41.18141\tr_loss: 35.24012\tkl_loss: 5.94129\n",
      "Epoch 068\tLoss: 41.17726\tr_loss: 35.22248\tkl_loss: 5.95478\n",
      "Epoch 069\tLoss: 41.16155\tr_loss: 35.19811\tkl_loss: 5.96344\n",
      "Epoch 070\tLoss: 41.12555\tr_loss: 35.16446\tkl_loss: 5.96110\n",
      "Epoch 071\tLoss: 41.10168\tr_loss: 35.12545\tkl_loss: 5.97622\n",
      "Epoch 072\tLoss: 41.05587\tr_loss: 35.09173\tkl_loss: 5.96415\n",
      "Epoch 073\tLoss: 41.02839\tr_loss: 35.04298\tkl_loss: 5.98541\n",
      "Epoch 074\tLoss: 41.02954\tr_loss: 35.03496\tkl_loss: 5.99458\n",
      "Epoch 075\tLoss: 40.98668\tr_loss: 34.99060\tkl_loss: 5.99608\n",
      "Epoch 076\tLoss: 40.94715\tr_loss: 34.95320\tkl_loss: 5.99395\n",
      "Epoch 077\tLoss: 40.93759\tr_loss: 34.93633\tkl_loss: 6.00126\n",
      "Epoch 078\tLoss: 40.88352\tr_loss: 34.87794\tkl_loss: 6.00557\n",
      "Epoch 079\tLoss: 40.88729\tr_loss: 34.87590\tkl_loss: 6.01139\n",
      "Epoch 080\tLoss: 40.85175\tr_loss: 34.83431\tkl_loss: 6.01744\n",
      "Epoch 081\tLoss: 40.82449\tr_loss: 34.80771\tkl_loss: 6.01678\n",
      "Epoch 082\tLoss: 40.81564\tr_loss: 34.79051\tkl_loss: 6.02512\n",
      "Epoch 083\tLoss: 40.79157\tr_loss: 34.76198\tkl_loss: 6.02959\n",
      "Epoch 084\tLoss: 40.78542\tr_loss: 34.74583\tkl_loss: 6.03959\n",
      "Epoch 085\tLoss: 40.72666\tr_loss: 34.68647\tkl_loss: 6.04019\n",
      "Epoch 086\tLoss: 40.72459\tr_loss: 34.68831\tkl_loss: 6.03628\n",
      "Epoch 087\tLoss: 40.69126\tr_loss: 34.63963\tkl_loss: 6.05163\n",
      "Epoch 088\tLoss: 40.69573\tr_loss: 34.63360\tkl_loss: 6.06213\n",
      "Epoch 089\tLoss: 40.65869\tr_loss: 34.60324\tkl_loss: 6.05545\n",
      "Epoch 090\tLoss: 40.64206\tr_loss: 34.57801\tkl_loss: 6.06405\n",
      "Epoch 091\tLoss: 40.61127\tr_loss: 34.54504\tkl_loss: 6.06623\n",
      "Epoch 092\tLoss: 40.60699\tr_loss: 34.52642\tkl_loss: 6.08057\n",
      "Epoch 093\tLoss: 40.57993\tr_loss: 34.50432\tkl_loss: 6.07561\n",
      "Epoch 094\tLoss: 40.56289\tr_loss: 34.48060\tkl_loss: 6.08229\n",
      "Epoch 095\tLoss: 40.51986\tr_loss: 34.44605\tkl_loss: 6.07381\n",
      "Epoch 096\tLoss: 40.51850\tr_loss: 34.43323\tkl_loss: 6.08527\n",
      "Epoch 097\tLoss: 40.48393\tr_loss: 34.39242\tkl_loss: 6.09151\n",
      "Epoch 098\tLoss: 40.47970\tr_loss: 34.38510\tkl_loss: 6.09460\n",
      "Epoch 099\tLoss: 40.47231\tr_loss: 34.37304\tkl_loss: 6.09927\n",
      "Epoch 100\tLoss: 40.46226\tr_loss: 34.35865\tkl_loss: 6.10360\n",
      "Epoch 101\tLoss: 40.41525\tr_loss: 34.31146\tkl_loss: 6.10379\n",
      "Epoch 102\tLoss: 40.39355\tr_loss: 34.28626\tkl_loss: 6.10729\n",
      "Epoch 103\tLoss: 40.39247\tr_loss: 34.27367\tkl_loss: 6.11880\n",
      "Epoch 104\tLoss: 40.36786\tr_loss: 34.24588\tkl_loss: 6.12198\n",
      "Epoch 105\tLoss: 40.34100\tr_loss: 34.22216\tkl_loss: 6.11884\n",
      "Epoch 106\tLoss: 40.32070\tr_loss: 34.20276\tkl_loss: 6.11793\n",
      "Epoch 107\tLoss: 40.32330\tr_loss: 34.19218\tkl_loss: 6.13112\n",
      "Epoch 108\tLoss: 40.29838\tr_loss: 34.16234\tkl_loss: 6.13604\n",
      "Epoch 109\tLoss: 40.27542\tr_loss: 34.14243\tkl_loss: 6.13299\n",
      "Epoch 110\tLoss: 40.27402\tr_loss: 34.13408\tkl_loss: 6.13994\n",
      "Epoch 111\tLoss: 40.23065\tr_loss: 34.08868\tkl_loss: 6.14196\n",
      "Epoch 112\tLoss: 40.24670\tr_loss: 34.09989\tkl_loss: 6.14680\n",
      "Epoch 113\tLoss: 40.22073\tr_loss: 34.06902\tkl_loss: 6.15171\n",
      "Epoch 114\tLoss: 40.21269\tr_loss: 34.04644\tkl_loss: 6.16625\n",
      "Epoch 115\tLoss: 40.18965\tr_loss: 34.02270\tkl_loss: 6.16695\n",
      "Epoch 116\tLoss: 40.16386\tr_loss: 34.01363\tkl_loss: 6.15023\n",
      "Epoch 117\tLoss: 40.14719\tr_loss: 33.98459\tkl_loss: 6.16260\n",
      "Epoch 118\tLoss: 40.15274\tr_loss: 33.98020\tkl_loss: 6.17254\n",
      "Epoch 119\tLoss: 40.13484\tr_loss: 33.95260\tkl_loss: 6.18225\n",
      "Epoch 120\tLoss: 40.11250\tr_loss: 33.92896\tkl_loss: 6.18354\n",
      "Epoch 121\tLoss: 40.09974\tr_loss: 33.91552\tkl_loss: 6.18422\n",
      "Epoch 122\tLoss: 40.07963\tr_loss: 33.88712\tkl_loss: 6.19251\n",
      "Epoch 123\tLoss: 40.04607\tr_loss: 33.87346\tkl_loss: 6.17261\n",
      "Epoch 124\tLoss: 40.02368\tr_loss: 33.83705\tkl_loss: 6.18662\n",
      "Epoch 125\tLoss: 40.02598\tr_loss: 33.84129\tkl_loss: 6.18469\n",
      "Epoch 126\tLoss: 39.99470\tr_loss: 33.80827\tkl_loss: 6.18643\n",
      "Epoch 127\tLoss: 40.01049\tr_loss: 33.81388\tkl_loss: 6.19660\n",
      "Epoch 128\tLoss: 39.98242\tr_loss: 33.78201\tkl_loss: 6.20041\n",
      "Epoch 129\tLoss: 39.97108\tr_loss: 33.76657\tkl_loss: 6.20451\n",
      "Epoch 130\tLoss: 39.95124\tr_loss: 33.74249\tkl_loss: 6.20875\n",
      "Epoch 131\tLoss: 39.93510\tr_loss: 33.73683\tkl_loss: 6.19827\n",
      "Epoch 132\tLoss: 39.92172\tr_loss: 33.71216\tkl_loss: 6.20956\n",
      "Epoch 133\tLoss: 39.91778\tr_loss: 33.70450\tkl_loss: 6.21327\n",
      "Epoch 134\tLoss: 39.89149\tr_loss: 33.67931\tkl_loss: 6.21218\n",
      "Epoch 135\tLoss: 39.89000\tr_loss: 33.66495\tkl_loss: 6.22505\n",
      "Epoch 136\tLoss: 39.87574\tr_loss: 33.65611\tkl_loss: 6.21963\n",
      "Epoch 137\tLoss: 39.87537\tr_loss: 33.64296\tkl_loss: 6.23240\n",
      "Epoch 138\tLoss: 39.82113\tr_loss: 33.58907\tkl_loss: 6.23206\n",
      "Epoch 139\tLoss: 39.84890\tr_loss: 33.60703\tkl_loss: 6.24187\n",
      "Epoch 140\tLoss: 39.83351\tr_loss: 33.59386\tkl_loss: 6.23965\n",
      "Epoch 141\tLoss: 39.81323\tr_loss: 33.57393\tkl_loss: 6.23930\n",
      "Epoch 142\tLoss: 39.78586\tr_loss: 33.55096\tkl_loss: 6.23491\n",
      "Epoch 143\tLoss: 39.79041\tr_loss: 33.54671\tkl_loss: 6.24370\n",
      "Epoch 144\tLoss: 39.77387\tr_loss: 33.52211\tkl_loss: 6.25177\n",
      "Epoch 145\tLoss: 39.76152\tr_loss: 33.51032\tkl_loss: 6.25120\n",
      "Epoch 146\tLoss: 39.73944\tr_loss: 33.49503\tkl_loss: 6.24441\n",
      "Epoch 147\tLoss: 39.74924\tr_loss: 33.48397\tkl_loss: 6.26528\n",
      "Epoch 148\tLoss: 39.72549\tr_loss: 33.46490\tkl_loss: 6.26060\n",
      "Epoch 149\tLoss: 39.71087\tr_loss: 33.45723\tkl_loss: 6.25364\n",
      "Epoch 150\tLoss: 39.70701\tr_loss: 33.43749\tkl_loss: 6.26952\n",
      "Epoch 151\tLoss: 39.68427\tr_loss: 33.41797\tkl_loss: 6.26629\n",
      "Epoch 152\tLoss: 39.69013\tr_loss: 33.40940\tkl_loss: 6.28073\n",
      "Epoch 153\tLoss: 39.67353\tr_loss: 33.39366\tkl_loss: 6.27987\n",
      "Epoch 154\tLoss: 39.65587\tr_loss: 33.37732\tkl_loss: 6.27855\n",
      "Epoch 155\tLoss: 39.65523\tr_loss: 33.36794\tkl_loss: 6.28729\n",
      "Epoch 156\tLoss: 39.62364\tr_loss: 33.34544\tkl_loss: 6.27820\n",
      "Epoch 157\tLoss: 39.62619\tr_loss: 33.34307\tkl_loss: 6.28312\n",
      "Epoch 158\tLoss: 39.62117\tr_loss: 33.33494\tkl_loss: 6.28623\n",
      "Epoch 159\tLoss: 39.59931\tr_loss: 33.31161\tkl_loss: 6.28770\n",
      "Epoch 160\tLoss: 39.58419\tr_loss: 33.29269\tkl_loss: 6.29150\n",
      "Epoch 161\tLoss: 39.59067\tr_loss: 33.28940\tkl_loss: 6.30127\n",
      "Epoch 162\tLoss: 39.56544\tr_loss: 33.27148\tkl_loss: 6.29397\n",
      "Epoch 163\tLoss: 39.56760\tr_loss: 33.26852\tkl_loss: 6.29909\n",
      "Epoch 164\tLoss: 39.56046\tr_loss: 33.25253\tkl_loss: 6.30793\n",
      "Epoch 165\tLoss: 39.54640\tr_loss: 33.23804\tkl_loss: 6.30836\n",
      "Epoch 166\tLoss: 39.53083\tr_loss: 33.22089\tkl_loss: 6.30994\n",
      "Epoch 167\tLoss: 39.51822\tr_loss: 33.20796\tkl_loss: 6.31026\n",
      "Epoch 168\tLoss: 39.51947\tr_loss: 33.20501\tkl_loss: 6.31446\n",
      "Epoch 169\tLoss: 39.48945\tr_loss: 33.18117\tkl_loss: 6.30828\n",
      "Epoch 170\tLoss: 39.49705\tr_loss: 33.17720\tkl_loss: 6.31985\n",
      "Epoch 171\tLoss: 39.47743\tr_loss: 33.15895\tkl_loss: 6.31848\n",
      "Epoch 172\tLoss: 39.46909\tr_loss: 33.15475\tkl_loss: 6.31434\n",
      "Epoch 173\tLoss: 39.45518\tr_loss: 33.14455\tkl_loss: 6.31063\n",
      "Epoch 174\tLoss: 39.43967\tr_loss: 33.11672\tkl_loss: 6.32295\n",
      "Epoch 175\tLoss: 39.45092\tr_loss: 33.11321\tkl_loss: 6.33771\n",
      "Epoch 176\tLoss: 39.45030\tr_loss: 33.10881\tkl_loss: 6.34149\n",
      "Epoch 177\tLoss: 39.42005\tr_loss: 33.08307\tkl_loss: 6.33698\n",
      "Epoch 178\tLoss: 39.41432\tr_loss: 33.08256\tkl_loss: 6.33176\n",
      "Epoch 179\tLoss: 39.42247\tr_loss: 33.07752\tkl_loss: 6.34495\n",
      "Epoch 180\tLoss: 39.40964\tr_loss: 33.06075\tkl_loss: 6.34889\n",
      "Epoch 181\tLoss: 39.38405\tr_loss: 33.04573\tkl_loss: 6.33831\n",
      "Epoch 182\tLoss: 39.39283\tr_loss: 33.04165\tkl_loss: 6.35118\n",
      "Epoch 183\tLoss: 39.37277\tr_loss: 33.02502\tkl_loss: 6.34775\n",
      "Epoch 184\tLoss: 39.36590\tr_loss: 33.01432\tkl_loss: 6.35159\n",
      "Epoch 185\tLoss: 39.36577\tr_loss: 33.00948\tkl_loss: 6.35630\n",
      "Epoch 186\tLoss: 39.33321\tr_loss: 32.98200\tkl_loss: 6.35122\n",
      "Epoch 187\tLoss: 39.33545\tr_loss: 32.98109\tkl_loss: 6.35436\n",
      "Epoch 188\tLoss: 39.34156\tr_loss: 32.97911\tkl_loss: 6.36246\n",
      "Epoch 189\tLoss: 39.31489\tr_loss: 32.95020\tkl_loss: 6.36468\n",
      "Epoch 190\tLoss: 39.33222\tr_loss: 32.96068\tkl_loss: 6.37154\n",
      "Epoch 191\tLoss: 39.31527\tr_loss: 32.94178\tkl_loss: 6.37349\n",
      "Epoch 192\tLoss: 39.29440\tr_loss: 32.91862\tkl_loss: 6.37579\n",
      "Epoch 193\tLoss: 39.29526\tr_loss: 32.92209\tkl_loss: 6.37317\n",
      "Epoch 194\tLoss: 39.27971\tr_loss: 32.90094\tkl_loss: 6.37876\n",
      "Epoch 195\tLoss: 39.28459\tr_loss: 32.90958\tkl_loss: 6.37501\n",
      "Epoch 196\tLoss: 39.25633\tr_loss: 32.88774\tkl_loss: 6.36859\n",
      "Epoch 197\tLoss: 39.25973\tr_loss: 32.88668\tkl_loss: 6.37305\n",
      "Epoch 198\tLoss: 39.25342\tr_loss: 32.87003\tkl_loss: 6.38339\n",
      "Epoch 199\tLoss: 39.23524\tr_loss: 32.85929\tkl_loss: 6.37595\n",
      "Epoch 200\tLoss: 39.22703\tr_loss: 32.85171\tkl_loss: 6.37532\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_r_loss = 0.0\n",
    "    running_kl_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs, mu, log_var = model(inputs)\n",
    "            r_loss = r_loss_factor * r_criterion(outputs, inputs)\n",
    "            kl_loss = vae_kl_loss(mu, log_var)\n",
    "            loss = r_loss + kl_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_r_loss += r_loss.item() * inputs.size(0)\n",
    "        running_kl_loss += kl_loss.item() * inputs.size(0)\n",
    "    \n",
    "    scheduler.step()    \n",
    "    epoch_loss = running_loss / dataset_size\n",
    "    epoch_r_loss = running_r_loss / dataset_size\n",
    "    epoch_kl_loss = running_kl_loss / dataset_size\n",
    "    print('Epoch {0:03d}\\tLoss: {1:0.5f}\\tr_loss: {2:0.5f}\\tkl_loss: {3:0.5f}'.format(\n",
    "        epoch + 1, epoch_loss, epoch_r_loss, epoch_kl_loss))\n",
    "\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86945038929e12337cb19f89597171695d197df89cc82498a4710d2d3ad0cdf4"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('gan_env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d96dd6218930269b4cd8feca07fdf09d8d639cf486d090b424fe3934a00b48b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
