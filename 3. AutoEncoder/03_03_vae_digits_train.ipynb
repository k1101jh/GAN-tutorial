{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data as data_utils\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from vae_auto_encoder import VAEAutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "learning_rate = 1e-3\n",
    "r_loss_factor = 1000\n",
    "decay_factor = 0.99\n",
    "model_save_path = \"./vae_digits_model.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(root='../data/',\n",
    "                            train=True,\n",
    "                            download=True,\n",
    "                            transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=4,\n",
    "                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAEAutoEncoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (6): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (8): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mu_layer): Linear(in_features=3136, out_features=2, bias=True)\n",
      "  (log_var_layer): Linear(in_features=3136, out_features=2, bias=True)\n",
      "  (linear): Linear(in_features=2, out_features=3136, bias=True)\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (2): ConvTranspose2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
      "    (3): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (4): ConvTranspose2d(64, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1), bias=False)\n",
      "    (5): LeakyReLU(negative_slope=0.01, inplace=True)\n",
      "    (6): ConvTranspose2d(32, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = VAEAutoEncoder(num_layers=4,\n",
    "                       encoder_channels=[1, 32, 64, 64, 64],\n",
    "                       encoder_kernel_sizes=[3, 3, 3, 3],\n",
    "                       encoder_strides=[1, 2, 2, 1],\n",
    "                       decoder_channels=[64, 64, 64, 32, 1],\n",
    "                       decoder_kernel_sizes=[3, 3, 3, 3],\n",
    "                       decoder_strides=[1, 2, 2, 1],\n",
    "                       linear_sizes=[3136, 2],\n",
    "                       view_size=[-1, 64, 7, 7],\n",
    "                       use_batch_norm=False,\n",
    "                       use_dropout=False).to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer,\n",
    "                                        lr_lambda=(lambda epoch: decay_factor ** epoch))\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_kl_loss(mu, log_var):\n",
    "    kl_loss = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp(), axis=1)\n",
    "    return torch.mean(kl_loss)\n",
    "\n",
    "r_criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000\tLoss: 56.65993\tr_loss: 53.36145\tkl_loss: 3.29848\n",
      "Epoch 001\tLoss: 49.31674\tr_loss: 44.86008\tkl_loss: 4.45666\n",
      "Epoch 002\tLoss: 47.59793\tr_loss: 42.84993\tkl_loss: 4.74800\n",
      "Epoch 003\tLoss: 46.70383\tr_loss: 41.78616\tkl_loss: 4.91766\n",
      "Epoch 004\tLoss: 46.00426\tr_loss: 40.95599\tkl_loss: 5.04827\n",
      "Epoch 005\tLoss: 45.61578\tr_loss: 40.46992\tkl_loss: 5.14586\n",
      "Epoch 006\tLoss: 45.21184\tr_loss: 39.98462\tkl_loss: 5.22722\n",
      "Epoch 007\tLoss: 44.92254\tr_loss: 39.64631\tkl_loss: 5.27623\n",
      "Epoch 008\tLoss: 44.63723\tr_loss: 39.32738\tkl_loss: 5.30985\n",
      "Epoch 009\tLoss: 44.38391\tr_loss: 39.02800\tkl_loss: 5.35591\n",
      "Epoch 010\tLoss: 44.24745\tr_loss: 38.85204\tkl_loss: 5.39541\n",
      "Epoch 011\tLoss: 44.03714\tr_loss: 38.61011\tkl_loss: 5.42703\n",
      "Epoch 012\tLoss: 43.88689\tr_loss: 38.41378\tkl_loss: 5.47310\n",
      "Epoch 013\tLoss: 43.69818\tr_loss: 38.20751\tkl_loss: 5.49067\n",
      "Epoch 014\tLoss: 43.60069\tr_loss: 38.09216\tkl_loss: 5.50853\n",
      "Epoch 015\tLoss: 43.44408\tr_loss: 37.91687\tkl_loss: 5.52721\n",
      "Epoch 016\tLoss: 43.34345\tr_loss: 37.78756\tkl_loss: 5.55588\n",
      "Epoch 017\tLoss: 43.24135\tr_loss: 37.67277\tkl_loss: 5.56859\n",
      "Epoch 018\tLoss: 43.14436\tr_loss: 37.55826\tkl_loss: 5.58610\n",
      "Epoch 019\tLoss: 43.02722\tr_loss: 37.43387\tkl_loss: 5.59334\n",
      "Epoch 020\tLoss: 42.95839\tr_loss: 37.33683\tkl_loss: 5.62156\n",
      "Epoch 021\tLoss: 42.85229\tr_loss: 37.22664\tkl_loss: 5.62565\n",
      "Epoch 022\tLoss: 42.76906\tr_loss: 37.11655\tkl_loss: 5.65251\n",
      "Epoch 023\tLoss: 42.65402\tr_loss: 37.00084\tkl_loss: 5.65318\n",
      "Epoch 024\tLoss: 42.60157\tr_loss: 36.93444\tkl_loss: 5.66712\n",
      "Epoch 025\tLoss: 42.52706\tr_loss: 36.85023\tkl_loss: 5.67683\n",
      "Epoch 026\tLoss: 42.47185\tr_loss: 36.77069\tkl_loss: 5.70115\n",
      "Epoch 027\tLoss: 42.39244\tr_loss: 36.69136\tkl_loss: 5.70108\n",
      "Epoch 028\tLoss: 42.33783\tr_loss: 36.62290\tkl_loss: 5.71493\n",
      "Epoch 029\tLoss: 42.27322\tr_loss: 36.54587\tkl_loss: 5.72734\n",
      "Epoch 030\tLoss: 42.21968\tr_loss: 36.48756\tkl_loss: 5.73213\n",
      "Epoch 031\tLoss: 42.16046\tr_loss: 36.41829\tkl_loss: 5.74217\n",
      "Epoch 032\tLoss: 42.12713\tr_loss: 36.36505\tkl_loss: 5.76208\n",
      "Epoch 033\tLoss: 42.05935\tr_loss: 36.29194\tkl_loss: 5.76741\n",
      "Epoch 034\tLoss: 41.98650\tr_loss: 36.22037\tkl_loss: 5.76612\n",
      "Epoch 035\tLoss: 41.97382\tr_loss: 36.20055\tkl_loss: 5.77327\n",
      "Epoch 036\tLoss: 41.91348\tr_loss: 36.12916\tkl_loss: 5.78432\n",
      "Epoch 037\tLoss: 41.85859\tr_loss: 36.06208\tkl_loss: 5.79651\n",
      "Epoch 038\tLoss: 41.84178\tr_loss: 36.03488\tkl_loss: 5.80690\n",
      "Epoch 039\tLoss: 41.79490\tr_loss: 35.98258\tkl_loss: 5.81232\n",
      "Epoch 040\tLoss: 41.72318\tr_loss: 35.90551\tkl_loss: 5.81767\n",
      "Epoch 041\tLoss: 41.70394\tr_loss: 35.88265\tkl_loss: 5.82129\n",
      "Epoch 042\tLoss: 41.61492\tr_loss: 35.78951\tkl_loss: 5.82542\n",
      "Epoch 043\tLoss: 41.61525\tr_loss: 35.78298\tkl_loss: 5.83227\n",
      "Epoch 044\tLoss: 41.56976\tr_loss: 35.73681\tkl_loss: 5.83294\n",
      "Epoch 045\tLoss: 41.52079\tr_loss: 35.67780\tkl_loss: 5.84299\n",
      "Epoch 046\tLoss: 41.49422\tr_loss: 35.64167\tkl_loss: 5.85255\n",
      "Epoch 047\tLoss: 41.44819\tr_loss: 35.57451\tkl_loss: 5.87368\n",
      "Epoch 048\tLoss: 41.43142\tr_loss: 35.55505\tkl_loss: 5.87637\n",
      "Epoch 049\tLoss: 41.40010\tr_loss: 35.52100\tkl_loss: 5.87910\n",
      "Epoch 050\tLoss: 41.34656\tr_loss: 35.47051\tkl_loss: 5.87605\n",
      "Epoch 051\tLoss: 41.32186\tr_loss: 35.43270\tkl_loss: 5.88915\n",
      "Epoch 052\tLoss: 41.31267\tr_loss: 35.41318\tkl_loss: 5.89949\n",
      "Epoch 053\tLoss: 41.26590\tr_loss: 35.36982\tkl_loss: 5.89608\n",
      "Epoch 054\tLoss: 41.24032\tr_loss: 35.33587\tkl_loss: 5.90445\n",
      "Epoch 055\tLoss: 41.20172\tr_loss: 35.29164\tkl_loss: 5.91009\n",
      "Epoch 056\tLoss: 41.17370\tr_loss: 35.26128\tkl_loss: 5.91242\n",
      "Epoch 057\tLoss: 41.13832\tr_loss: 35.22469\tkl_loss: 5.91363\n",
      "Epoch 058\tLoss: 41.11318\tr_loss: 35.18293\tkl_loss: 5.93024\n",
      "Epoch 059\tLoss: 41.06705\tr_loss: 35.13694\tkl_loss: 5.93011\n",
      "Epoch 060\tLoss: 41.05303\tr_loss: 35.11933\tkl_loss: 5.93371\n",
      "Epoch 061\tLoss: 41.02936\tr_loss: 35.08755\tkl_loss: 5.94182\n",
      "Epoch 062\tLoss: 40.99417\tr_loss: 35.04505\tkl_loss: 5.94911\n",
      "Epoch 063\tLoss: 40.96366\tr_loss: 35.01202\tkl_loss: 5.95164\n",
      "Epoch 064\tLoss: 40.95693\tr_loss: 35.00369\tkl_loss: 5.95324\n",
      "Epoch 065\tLoss: 40.92204\tr_loss: 34.96042\tkl_loss: 5.96161\n",
      "Epoch 066\tLoss: 40.87332\tr_loss: 34.91992\tkl_loss: 5.95339\n",
      "Epoch 067\tLoss: 40.87129\tr_loss: 34.90612\tkl_loss: 5.96517\n",
      "Epoch 068\tLoss: 40.84034\tr_loss: 34.86619\tkl_loss: 5.97414\n",
      "Epoch 069\tLoss: 40.79944\tr_loss: 34.81623\tkl_loss: 5.98321\n",
      "Epoch 070\tLoss: 40.79323\tr_loss: 34.81813\tkl_loss: 5.97510\n",
      "Epoch 071\tLoss: 40.78048\tr_loss: 34.78647\tkl_loss: 5.99401\n",
      "Epoch 072\tLoss: 40.73196\tr_loss: 34.74981\tkl_loss: 5.98215\n",
      "Epoch 073\tLoss: 40.75260\tr_loss: 34.75118\tkl_loss: 6.00141\n",
      "Epoch 074\tLoss: 40.68828\tr_loss: 34.68660\tkl_loss: 6.00167\n",
      "Epoch 075\tLoss: 40.69834\tr_loss: 34.68894\tkl_loss: 6.00940\n",
      "Epoch 076\tLoss: 40.64082\tr_loss: 34.62983\tkl_loss: 6.01099\n",
      "Epoch 077\tLoss: 40.63707\tr_loss: 34.61916\tkl_loss: 6.01791\n",
      "Epoch 078\tLoss: 40.62277\tr_loss: 34.59717\tkl_loss: 6.02560\n",
      "Epoch 079\tLoss: 40.58303\tr_loss: 34.55239\tkl_loss: 6.03064\n",
      "Epoch 080\tLoss: 40.58093\tr_loss: 34.55442\tkl_loss: 6.02651\n",
      "Epoch 081\tLoss: 40.54451\tr_loss: 34.50996\tkl_loss: 6.03456\n",
      "Epoch 082\tLoss: 40.53237\tr_loss: 34.49539\tkl_loss: 6.03698\n",
      "Epoch 083\tLoss: 40.50989\tr_loss: 34.46459\tkl_loss: 6.04530\n",
      "Epoch 084\tLoss: 40.48869\tr_loss: 34.44791\tkl_loss: 6.04079\n",
      "Epoch 085\tLoss: 40.46352\tr_loss: 34.41174\tkl_loss: 6.05178\n",
      "Epoch 086\tLoss: 40.42513\tr_loss: 34.38423\tkl_loss: 6.04091\n",
      "Epoch 087\tLoss: 40.42259\tr_loss: 34.37088\tkl_loss: 6.05171\n",
      "Epoch 088\tLoss: 40.38532\tr_loss: 34.32947\tkl_loss: 6.05585\n",
      "Epoch 089\tLoss: 40.39420\tr_loss: 34.33491\tkl_loss: 6.05929\n",
      "Epoch 090\tLoss: 40.35558\tr_loss: 34.29070\tkl_loss: 6.06488\n",
      "Epoch 091\tLoss: 40.35778\tr_loss: 34.28925\tkl_loss: 6.06853\n",
      "Epoch 092\tLoss: 40.31616\tr_loss: 34.24744\tkl_loss: 6.06871\n",
      "Epoch 093\tLoss: 40.32535\tr_loss: 34.24755\tkl_loss: 6.07780\n",
      "Epoch 094\tLoss: 40.27588\tr_loss: 34.19774\tkl_loss: 6.07814\n",
      "Epoch 095\tLoss: 40.28457\tr_loss: 34.19461\tkl_loss: 6.08996\n",
      "Epoch 096\tLoss: 40.24256\tr_loss: 34.17157\tkl_loss: 6.07099\n",
      "Epoch 097\tLoss: 40.24092\tr_loss: 34.15366\tkl_loss: 6.08726\n",
      "Epoch 098\tLoss: 40.20448\tr_loss: 34.11593\tkl_loss: 6.08855\n",
      "Epoch 099\tLoss: 40.20327\tr_loss: 34.09821\tkl_loss: 6.10506\n",
      "Epoch 100\tLoss: 40.17472\tr_loss: 34.07632\tkl_loss: 6.09840\n",
      "Epoch 101\tLoss: 40.17704\tr_loss: 34.07271\tkl_loss: 6.10433\n",
      "Epoch 102\tLoss: 40.13477\tr_loss: 34.02368\tkl_loss: 6.11109\n",
      "Epoch 103\tLoss: 40.14707\tr_loss: 34.03082\tkl_loss: 6.11625\n",
      "Epoch 104\tLoss: 40.10381\tr_loss: 33.99695\tkl_loss: 6.10686\n",
      "Epoch 105\tLoss: 40.11922\tr_loss: 33.99829\tkl_loss: 6.12092\n",
      "Epoch 106\tLoss: 40.10048\tr_loss: 33.97373\tkl_loss: 6.12675\n",
      "Epoch 107\tLoss: 40.06130\tr_loss: 33.93765\tkl_loss: 6.12366\n",
      "Epoch 108\tLoss: 40.04646\tr_loss: 33.92130\tkl_loss: 6.12516\n",
      "Epoch 109\tLoss: 40.04604\tr_loss: 33.90535\tkl_loss: 6.14069\n",
      "Epoch 110\tLoss: 40.03273\tr_loss: 33.89787\tkl_loss: 6.13486\n",
      "Epoch 111\tLoss: 40.01061\tr_loss: 33.87296\tkl_loss: 6.13765\n",
      "Epoch 112\tLoss: 39.99659\tr_loss: 33.85863\tkl_loss: 6.13796\n",
      "Epoch 113\tLoss: 39.97964\tr_loss: 33.84282\tkl_loss: 6.13682\n",
      "Epoch 114\tLoss: 39.96300\tr_loss: 33.81975\tkl_loss: 6.14324\n",
      "Epoch 115\tLoss: 39.95774\tr_loss: 33.80478\tkl_loss: 6.15296\n",
      "Epoch 116\tLoss: 39.93882\tr_loss: 33.78238\tkl_loss: 6.15644\n",
      "Epoch 117\tLoss: 39.93787\tr_loss: 33.77754\tkl_loss: 6.16033\n",
      "Epoch 118\tLoss: 39.91612\tr_loss: 33.75002\tkl_loss: 6.16611\n",
      "Epoch 119\tLoss: 39.88833\tr_loss: 33.73144\tkl_loss: 6.15689\n",
      "Epoch 120\tLoss: 39.88804\tr_loss: 33.71870\tkl_loss: 6.16934\n",
      "Epoch 121\tLoss: 39.86569\tr_loss: 33.69803\tkl_loss: 6.16766\n",
      "Epoch 122\tLoss: 39.83612\tr_loss: 33.67732\tkl_loss: 6.15880\n",
      "Epoch 123\tLoss: 39.84735\tr_loss: 33.66739\tkl_loss: 6.17997\n",
      "Epoch 124\tLoss: 39.83279\tr_loss: 33.65102\tkl_loss: 6.18177\n",
      "Epoch 125\tLoss: 39.81743\tr_loss: 33.64033\tkl_loss: 6.17710\n",
      "Epoch 126\tLoss: 39.80648\tr_loss: 33.61933\tkl_loss: 6.18715\n",
      "Epoch 127\tLoss: 39.78232\tr_loss: 33.59700\tkl_loss: 6.18532\n",
      "Epoch 128\tLoss: 39.77518\tr_loss: 33.58993\tkl_loss: 6.18525\n",
      "Epoch 129\tLoss: 39.76822\tr_loss: 33.57875\tkl_loss: 6.18947\n",
      "Epoch 130\tLoss: 39.74422\tr_loss: 33.55909\tkl_loss: 6.18512\n",
      "Epoch 131\tLoss: 39.73913\tr_loss: 33.54464\tkl_loss: 6.19448\n",
      "Epoch 132\tLoss: 39.71877\tr_loss: 33.52582\tkl_loss: 6.19295\n",
      "Epoch 133\tLoss: 39.72973\tr_loss: 33.52561\tkl_loss: 6.20412\n",
      "Epoch 134\tLoss: 39.68565\tr_loss: 33.48748\tkl_loss: 6.19817\n",
      "Epoch 135\tLoss: 39.70426\tr_loss: 33.48322\tkl_loss: 6.22105\n",
      "Epoch 136\tLoss: 39.66640\tr_loss: 33.46250\tkl_loss: 6.20390\n",
      "Epoch 137\tLoss: 39.66149\tr_loss: 33.45642\tkl_loss: 6.20507\n",
      "Epoch 138\tLoss: 39.67798\tr_loss: 33.44695\tkl_loss: 6.23103\n",
      "Epoch 139\tLoss: 39.63346\tr_loss: 33.41993\tkl_loss: 6.21353\n",
      "Epoch 140\tLoss: 39.60959\tr_loss: 33.40101\tkl_loss: 6.20858\n",
      "Epoch 141\tLoss: 39.63155\tr_loss: 33.40700\tkl_loss: 6.22454\n",
      "Epoch 142\tLoss: 39.60128\tr_loss: 33.37592\tkl_loss: 6.22535\n",
      "Epoch 143\tLoss: 39.60279\tr_loss: 33.36575\tkl_loss: 6.23704\n",
      "Epoch 144\tLoss: 39.57816\tr_loss: 33.35073\tkl_loss: 6.22743\n",
      "Epoch 145\tLoss: 39.56963\tr_loss: 33.34526\tkl_loss: 6.22437\n",
      "Epoch 146\tLoss: 39.55819\tr_loss: 33.32338\tkl_loss: 6.23482\n",
      "Epoch 147\tLoss: 39.53420\tr_loss: 33.30409\tkl_loss: 6.23010\n",
      "Epoch 148\tLoss: 39.54947\tr_loss: 33.30530\tkl_loss: 6.24417\n",
      "Epoch 149\tLoss: 39.53429\tr_loss: 33.29189\tkl_loss: 6.24240\n",
      "Epoch 150\tLoss: 39.51942\tr_loss: 33.26864\tkl_loss: 6.25078\n",
      "Epoch 151\tLoss: 39.50079\tr_loss: 33.25817\tkl_loss: 6.24262\n",
      "Epoch 152\tLoss: 39.49518\tr_loss: 33.24091\tkl_loss: 6.25427\n",
      "Epoch 153\tLoss: 39.50361\tr_loss: 33.24585\tkl_loss: 6.25776\n",
      "Epoch 154\tLoss: 39.47471\tr_loss: 33.21233\tkl_loss: 6.26238\n",
      "Epoch 155\tLoss: 39.47237\tr_loss: 33.21450\tkl_loss: 6.25787\n",
      "Epoch 156\tLoss: 39.45844\tr_loss: 33.19529\tkl_loss: 6.26315\n",
      "Epoch 157\tLoss: 39.45329\tr_loss: 33.19479\tkl_loss: 6.25850\n",
      "Epoch 158\tLoss: 39.42756\tr_loss: 33.16669\tkl_loss: 6.26087\n",
      "Epoch 159\tLoss: 39.43185\tr_loss: 33.16283\tkl_loss: 6.26902\n",
      "Epoch 160\tLoss: 39.42882\tr_loss: 33.15187\tkl_loss: 6.27695\n",
      "Epoch 161\tLoss: 39.39794\tr_loss: 33.13332\tkl_loss: 6.26462\n",
      "Epoch 162\tLoss: 39.40152\tr_loss: 33.13105\tkl_loss: 6.27047\n",
      "Epoch 163\tLoss: 39.38987\tr_loss: 33.11341\tkl_loss: 6.27646\n",
      "Epoch 164\tLoss: 39.37625\tr_loss: 33.10159\tkl_loss: 6.27466\n",
      "Epoch 165\tLoss: 39.36358\tr_loss: 33.08532\tkl_loss: 6.27825\n",
      "Epoch 166\tLoss: 39.35609\tr_loss: 33.07487\tkl_loss: 6.28121\n",
      "Epoch 167\tLoss: 39.35557\tr_loss: 33.07124\tkl_loss: 6.28433\n",
      "Epoch 168\tLoss: 39.34177\tr_loss: 33.05216\tkl_loss: 6.28961\n",
      "Epoch 169\tLoss: 39.32804\tr_loss: 33.04346\tkl_loss: 6.28458\n",
      "Epoch 170\tLoss: 39.31653\tr_loss: 33.03365\tkl_loss: 6.28289\n",
      "Epoch 171\tLoss: 39.30806\tr_loss: 33.01999\tkl_loss: 6.28807\n",
      "Epoch 172\tLoss: 39.31618\tr_loss: 33.00850\tkl_loss: 6.30768\n",
      "Epoch 173\tLoss: 39.30129\tr_loss: 32.99885\tkl_loss: 6.30244\n",
      "Epoch 174\tLoss: 39.29285\tr_loss: 32.99305\tkl_loss: 6.29980\n",
      "Epoch 175\tLoss: 39.28809\tr_loss: 32.98725\tkl_loss: 6.30084\n",
      "Epoch 176\tLoss: 39.27627\tr_loss: 32.96181\tkl_loss: 6.31446\n",
      "Epoch 177\tLoss: 39.26782\tr_loss: 32.95614\tkl_loss: 6.31168\n",
      "Epoch 178\tLoss: 39.25477\tr_loss: 32.94717\tkl_loss: 6.30760\n",
      "Epoch 179\tLoss: 39.23642\tr_loss: 32.92364\tkl_loss: 6.31278\n",
      "Epoch 180\tLoss: 39.24186\tr_loss: 32.92664\tkl_loss: 6.31522\n",
      "Epoch 181\tLoss: 39.23385\tr_loss: 32.91930\tkl_loss: 6.31455\n",
      "Epoch 182\tLoss: 39.22301\tr_loss: 32.90244\tkl_loss: 6.32057\n",
      "Epoch 183\tLoss: 39.21313\tr_loss: 32.89627\tkl_loss: 6.31685\n",
      "Epoch 184\tLoss: 39.20333\tr_loss: 32.88345\tkl_loss: 6.31988\n",
      "Epoch 185\tLoss: 39.19862\tr_loss: 32.87910\tkl_loss: 6.31951\n",
      "Epoch 186\tLoss: 39.19427\tr_loss: 32.86980\tkl_loss: 6.32447\n",
      "Epoch 187\tLoss: 39.17754\tr_loss: 32.84863\tkl_loss: 6.32891\n",
      "Epoch 188\tLoss: 39.16453\tr_loss: 32.84491\tkl_loss: 6.31962\n",
      "Epoch 189\tLoss: 39.17030\tr_loss: 32.83987\tkl_loss: 6.33043\n",
      "Epoch 190\tLoss: 39.15395\tr_loss: 32.82770\tkl_loss: 6.32625\n",
      "Epoch 191\tLoss: 39.16062\tr_loss: 32.81557\tkl_loss: 6.34505\n",
      "Epoch 192\tLoss: 39.15145\tr_loss: 32.81341\tkl_loss: 6.33804\n",
      "Epoch 193\tLoss: 39.14340\tr_loss: 32.79957\tkl_loss: 6.34384\n",
      "Epoch 194\tLoss: 39.12723\tr_loss: 32.78938\tkl_loss: 6.33785\n",
      "Epoch 195\tLoss: 39.12693\tr_loss: 32.77653\tkl_loss: 6.35040\n",
      "Epoch 196\tLoss: 39.12449\tr_loss: 32.78137\tkl_loss: 6.34311\n",
      "Epoch 197\tLoss: 39.10068\tr_loss: 32.75617\tkl_loss: 6.34451\n",
      "Epoch 198\tLoss: 39.09896\tr_loss: 32.74973\tkl_loss: 6.34923\n",
      "Epoch 199\tLoss: 39.07244\tr_loss: 32.72829\tkl_loss: 6.34415\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    running_r_loss = 0.0\n",
    "    running_kl_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with torch.set_grad_enabled(True):\n",
    "            outputs, mu, log_var = model(inputs)\n",
    "            r_loss = r_loss_factor * r_criterion(outputs, inputs)\n",
    "            kl_loss = vae_kl_loss(mu, log_var)\n",
    "            loss = r_loss + kl_loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_r_loss += r_loss.item() * inputs.size(0)\n",
    "        running_kl_loss += kl_loss.item() * inputs.size(0)\n",
    "    \n",
    "    scheduler.step()    \n",
    "    epoch_loss = running_loss / len(train_data)\n",
    "    epoch_r_loss = running_r_loss / len(train_data)\n",
    "    epoch_kl_loss = running_kl_loss / len(train_data)\n",
    "    print('Epoch {0:03d}\\tLoss: {1:0.5f}\\tr_loss: {2:0.5f}\\tkl_loss: {3:0.5f}'.format(\n",
    "        epoch, epoch_loss, epoch_r_loss, epoch_kl_loss))\n",
    "\n",
    "    torch.save(model.state_dict(), model_save_path)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "86945038929e12337cb19f89597171695d197df89cc82498a4710d2d3ad0cdf4"
  },
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('gan_env': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
